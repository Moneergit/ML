{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `itmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(29, 1),  y.shape=(29,)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    filename = \"Data/itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\n",
      "\n",
      "lin.reg.score(X, y)=0.73\n",
      "    MLP.score(X, y)=-4.58\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "def isNumpyData(t: np.ndarray, expected_ndim: int):\n",
    "    assert isinstance(expected_ndim, int), f\"input parameter 'expected_ndim' is not an integer but a '{type(expected_ndim)}'\"\n",
    "    assert expected_ndim>=0, f\"expected input parameter 'expected_ndim' to be >=0, got {expected_ndim}\"\n",
    "    if t is None:\n",
    "        print(\"input parameter 't' is None\", file=sys.stderr)\n",
    "        return False\n",
    "    if not isinstance(t, np.ndarray):\n",
    "        print(\"excepted numpy.ndarray got type '{type(t)}'\", file=sys.stderr)\n",
    "        return False\n",
    "    if not t.ndim==expected_ndim:\n",
    "        print(\"expected ndim={expected_ndim} but found {t.ndim}\", file=sys.stderr)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def PlotModels(model1, model2, X: np.ndarray, y: np.ndarray, name_model1: str, name_model2: str):\n",
    "    \n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model, X: np.ndarray, y: np.ndarray,):\n",
    "        assert isNumpyData(X, 2) and isNumpyData(y, 1) and X.shape[0]==y.shape[0]\n",
    "        y_pred_model = model.predict(X)\n",
    "        score_model = r2_score(y, y_pred_model) # call r2\n",
    "        return y_pred_model, score_model    \n",
    "\n",
    "    assert isinstance(name_model1, str) and isinstance(name_model2, str)\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(model1, X, y)\n",
    "    y_pred_model2, score_model2 = CalcPredAndScore(model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{(name_model1).rjust(l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{(name_model2).rjust(l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y, \"lin.reg\", \"MLP\")\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(29, 1), y.shape=(29,)\n",
      "R² score after scaling: 0.6658908390645877\n"
     ]
    }
   ],
   "source": [
    "# TODO: add your code here..\n",
    "# assert False, \"TODO: rescale X and refit the model(s)..\" \n",
    "\n",
    "\n",
    "# Step 1: Load the data (you have this already in the file, so no need to redo this part)\n",
    "import pickle\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    filename = \"Data/itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "print(f\"X.shape={X.shape}, y.shape={y.shape}\")\n",
    "\n",
    "# Step 2: Min-Max Scaling the data manually (to [0, 1])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Train the MLP with scaled data\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize and train the MLP regressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_scaled, y)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_scaled)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"R² score after scaling: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score after scaling with pipeline: 0.6658908390645877\n"
     ]
    }
   ],
   "source": [
    "# TODO: add your code here..\n",
    "#assert False, \"TODO: put everything into a pipeline..\"\n",
    "\n",
    "# Step 1: Import necessary modules\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Step 2: Create the pipeline with MinMaxScaler and MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),      # First step: Scale the features\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42))  # Second step: MLP model\n",
    "])\n",
    "\n",
    "# Step 3: Train the pipeline\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "# Step 5: Calculate the R² score\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Output the result\n",
    "print(f\"R² score after scaling with pipeline: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score with Min-Max Scaler: 0.6658908390645877\n",
      "R² score with Standard Scaler: 0.7109620813704723\n"
     ]
    }
   ],
   "source": [
    "# TODO: research the problem here..\n",
    "#assert False, \"TODO: investigate outlier problems and try a StandardScaler..\" \n",
    "\n",
    "# Step 1: Import the Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Create and train models with both scalers\n",
    "# Min-Max Scaler Pipeline\n",
    "pipeline_min_max = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42))\n",
    "])\n",
    "pipeline_min_max.fit(X, y)\n",
    "y_pred_min_max = pipeline_min_max.predict(X)\n",
    "r2_min_max = r2_score(y, y_pred_min_max)\n",
    "\n",
    "# Standard Scaler Pipeline\n",
    "pipeline_standard = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42))\n",
    "])\n",
    "pipeline_standard.fit(X, y)\n",
    "y_pred_standard = pipeline_standard.predict(X)\n",
    "r2_standard = r2_score(y, y_pred_standard)\n",
    "\n",
    "# Output the R² scores\n",
    "print(f\"R² score with Min-Max Scaler: {r2_min_max}\")\n",
    "print(f\"R² score with Standard Scaler: {r2_standard}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (pp.333-334 2nd./pp.358-359 3rd. [HOML]) and the ELU activation function (p.336 2nd./ p.363 3rd. [HOML]).\n",
    "\n",
    "[OPTIONAL 1]: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "[OPTIONAL 2]: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "[OPTIONAL 3]: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: test out various hyperparameters for the MLP..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02-10| CEF, updated for SWMAL F22.\n",
    "2023-02-19| CEF, updated for SWMAL F23, adjuste page numbers for 3rd.ed.\n",
    "2023-02-21| CEF, added types, rewrote CalcPredAndScore and added isNumpyData.\n",
    "2024-09-11| CEF, updated page refefences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
